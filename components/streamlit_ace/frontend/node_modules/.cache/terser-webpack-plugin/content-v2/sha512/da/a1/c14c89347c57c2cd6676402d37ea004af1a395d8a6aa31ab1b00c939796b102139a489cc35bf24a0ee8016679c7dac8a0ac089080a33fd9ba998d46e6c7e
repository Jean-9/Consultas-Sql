{"code":"ace.define(\"ace/mode/json_highlight_rules\",[\"require\",\"exports\",\"module\",\"ace/lib/oop\",\"ace/mode/text_highlight_rules\"],function(e,t,n){\"use strict\";var o=e(\"../lib/oop\"),r=e(\"./text_highlight_rules\").TextHighlightRules,i=function(){this.$rules={start:[{token:\"variable\",regex:'[\"](?:(?:\\\\\\\\.)|(?:[^\"\\\\\\\\]))*?[\"]\\\\s*(?=:)'},{token:\"string\",regex:'\"',next:\"string\"},{token:\"constant.numeric\",regex:\"0[xX][0-9a-fA-F]+\\\\b\"},{token:\"constant.numeric\",regex:\"[+-]?\\\\d+(?:(?:\\\\.\\\\d*)?(?:[eE][+-]?\\\\d+)?)?\\\\b\"},{token:\"constant.language.boolean\",regex:\"(?:true|false)\\\\b\"},{token:\"text\",regex:\"['](?:(?:\\\\\\\\.)|(?:[^'\\\\\\\\]))*?[']\"},{token:\"comment\",regex:\"\\\\/\\\\/.*$\"},{token:\"comment.start\",regex:\"\\\\/\\\\*\",next:\"comment\"},{token:\"paren.lparen\",regex:\"[[({]\"},{token:\"paren.rparen\",regex:\"[\\\\])}]\"},{token:\"punctuation.operator\",regex:/[,]/},{token:\"text\",regex:\"\\\\s+\"}],string:[{token:\"constant.language.escape\",regex:/\\\\(?:x[0-9a-fA-F]{2}|u[0-9a-fA-F]{4}|[\"\\\\\\/bfnrt])/},{token:\"string\",regex:'\"|$',next:\"start\"},{defaultToken:\"string\"}],comment:[{token:\"comment.end\",regex:\"\\\\*\\\\/\",next:\"start\"},{defaultToken:\"comment\"}]}};o.inherits(i,r),t.JsonHighlightRules=i}),ace.define(\"ace/ext/simple_tokenizer\",[\"require\",\"exports\",\"module\",\"ace/ext/simple_tokenizer\",\"ace/mode/json_highlight_rules\",\"ace/tokenizer\",\"ace/layer/text_util\"],function(e,t,n){\"use strict\";var o=e(\"../tokenizer\").Tokenizer,r=e(\"../layer/text_util\").isTextToken,i=function(){function e(e,t){this._lines=e.split(/\\r\\n|\\r|\\n/),this._states=[],this._tokenizer=t}return e.prototype.getTokens=function(e){var t=this._lines[e],n=this._states[e-1],o=this._tokenizer.getLineTokens(t,n);return this._states[e]=o.state,o.tokens},e.prototype.getLength=function(){return this._lines.length},e}();t.tokenize=function(e,t){for(var n=new i(e,new o(t.getRules())),s=[],a=0;a<n.getLength();a++){var u=n.getTokens(a);s.push(u.map(function(e){return{className:r(e.type)?void 0:\"ace_\"+e.type.replace(/\\./g,\" ace_\"),value:e.value}}))}return s}}),ace.require([\"ace/ext/simple_tokenizer\"],function(e){\"object\"==typeof module&&\"object\"==typeof exports&&module&&(module.exports=e)});","name":"3e6f2f8a5ff74b35bd58c45a5182637c.js","input":"ace.define(\"ace/mode/json_highlight_rules\", [\"require\", \"exports\", \"module\", \"ace/lib/oop\", \"ace/mode/text_highlight_rules\"], function (require, exports, module) {\n  \"use strict\";\n\n  var oop = require(\"../lib/oop\");\n  var TextHighlightRules = require(\"./text_highlight_rules\").TextHighlightRules;\n  var JsonHighlightRules = function () {\n    this.$rules = {\n      \"start\": [{\n        token: \"variable\",\n        // single line\n        regex: '[\"](?:(?:\\\\\\\\.)|(?:[^\"\\\\\\\\]))*?[\"]\\\\s*(?=:)'\n      }, {\n        token: \"string\",\n        // single line\n        regex: '\"',\n        next: \"string\"\n      }, {\n        token: \"constant.numeric\",\n        // hex\n        regex: \"0[xX][0-9a-fA-F]+\\\\b\"\n      }, {\n        token: \"constant.numeric\",\n        // float\n        regex: \"[+-]?\\\\d+(?:(?:\\\\.\\\\d*)?(?:[eE][+-]?\\\\d+)?)?\\\\b\"\n      }, {\n        token: \"constant.language.boolean\",\n        regex: \"(?:true|false)\\\\b\"\n      }, {\n        token: \"text\",\n        // single quoted strings are not allowed\n        regex: \"['](?:(?:\\\\\\\\.)|(?:[^'\\\\\\\\]))*?[']\"\n      }, {\n        token: \"comment\",\n        // comments are not allowed, but who cares?\n        regex: \"\\\\/\\\\/.*$\"\n      }, {\n        token: \"comment.start\",\n        // comments are not allowed, but who cares?\n        regex: \"\\\\/\\\\*\",\n        next: \"comment\"\n      }, {\n        token: \"paren.lparen\",\n        regex: \"[[({]\"\n      }, {\n        token: \"paren.rparen\",\n        regex: \"[\\\\])}]\"\n      }, {\n        token: \"punctuation.operator\",\n        regex: /[,]/\n      }, {\n        token: \"text\",\n        regex: \"\\\\s+\"\n      }],\n      \"string\": [{\n        token: \"constant.language.escape\",\n        regex: /\\\\(?:x[0-9a-fA-F]{2}|u[0-9a-fA-F]{4}|[\"\\\\\\/bfnrt])/\n      }, {\n        token: \"string\",\n        regex: '\"|$',\n        next: \"start\"\n      }, {\n        defaultToken: \"string\"\n      }],\n      \"comment\": [{\n        token: \"comment.end\",\n        // comments are not allowed, but who cares?\n        regex: \"\\\\*\\\\/\",\n        next: \"start\"\n      }, {\n        defaultToken: \"comment\"\n      }]\n    };\n  };\n  oop.inherits(JsonHighlightRules, TextHighlightRules);\n  exports.JsonHighlightRules = JsonHighlightRules;\n});\nace.define(\"ace/ext/simple_tokenizer\", [\"require\", \"exports\", \"module\", \"ace/ext/simple_tokenizer\", \"ace/mode/json_highlight_rules\", \"ace/tokenizer\", \"ace/layer/text_util\"], function (require, exports, module) {\n  /**\n  * ## Simple tokenizer extension\n  *\n  * Provides standalone tokenization functionality that can parse code content using Ace's highlight rules without\n  * requiring a full editor instance. This is useful for generating syntax-highlighted tokens for external rendering,\n  * static code generation, or testing tokenization rules. The tokenizer processes text line by line and returns\n  * structured token data with CSS class names compatible with Ace themes.\n  *\n  * **Usage:**\n  * ```javascript\n  * const { tokenize } = require(\"ace/ext/simple_tokenizer\");\n  * const { JsonHighlightRules } = require(\"ace/mode/json_highlight_rules\");\n  *\n  * const content = '{\"name\": \"value\"}';\n  * const tokens = tokenize(content, new JsonHighlightRules());\n  * // Returns: [[{className: \"ace_paren ace_lparen\", value: \"{\"}, ...]]\n  * ```\n  *\n  * @module\n  */\n  \"use strict\";\n\n  var Tokenizer = require(\"../tokenizer\").Tokenizer;\n  var isTextToken = require(\"../layer/text_util\").isTextToken;\n  var SimpleTokenizer = /** @class */function () {\n    function SimpleTokenizer(content, tokenizer) {\n      this._lines = content.split(/\\r\\n|\\r|\\n/);\n      this._states = [];\n      this._tokenizer = tokenizer;\n    }\n    SimpleTokenizer.prototype.getTokens = function (row) {\n      var line = this._lines[row];\n      var previousState = this._states[row - 1];\n      var data = this._tokenizer.getLineTokens(line, previousState);\n      this._states[row] = data.state;\n      return data.tokens;\n    };\n    SimpleTokenizer.prototype.getLength = function () {\n      return this._lines.length;\n    };\n    return SimpleTokenizer;\n  }();\n  function tokenize(content, highlightRules) {\n    var tokenizer = new SimpleTokenizer(content, new Tokenizer(highlightRules.getRules()));\n    var result = [];\n    for (var lineIndex = 0; lineIndex < tokenizer.getLength(); lineIndex++) {\n      var lineTokens = tokenizer.getTokens(lineIndex);\n      result.push(lineTokens.map(function (token) {\n        return {\n          className: isTextToken(token.type) ? undefined : \"ace_\" + token.type.replace(/\\./g, \" ace_\"),\n          value: token.value\n        };\n      }));\n    }\n    return result;\n  }\n  exports.tokenize = tokenize;\n});\n(function () {\n  ace.require([\"ace/ext/simple_tokenizer\"], function (m) {\n    if (typeof module == \"object\" && typeof exports == \"object\" && module) {\n      module.exports = m;\n    }\n  });\n})();"}